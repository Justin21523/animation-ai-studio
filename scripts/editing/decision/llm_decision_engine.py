"""
LLM Decision Engine for AI-Driven Video Editing

CORE INNOVATION: LLM makes ALL editing decisions autonomously.

The LLM analyzes video content and decides:
- Which clips to cut
- How to arrange scenes
- What effects to apply
- Speed changes and timing
- Quality thresholds for iteration

This is the "brain" of the autonomous video editing system.

Integration:
- Uses Module 7 (Video Analysis) for content understanding
- Uses Module 8 editing tools for execution
- Uses Agent Framework for reasoning and decision-making

Author: Animation AI Studio
Date: 2025-11-17
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import time
import json
import asyncio

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.core.llm_client import LLMClient


logger = logging.getLogger(__name__)


@dataclass
class EditDecision:
    """Single editing decision made by LLM"""
    decision_id: str
    decision_type: str  # "cut", "speed", "composite", "effect", "transition"
    confidence: float  # 0.0 to 1.0
    reasoning: str  # LLM's reasoning for this decision
    parameters: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5  # 1-10, higher = more important
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "decision_id": self.decision_id,
            "decision_type": self.decision_type,
            "confidence": self.confidence,
            "reasoning": self.reasoning,
            "parameters": self.parameters,
            "priority": self.priority,
            "metadata": self.metadata
        }


@dataclass
class EditPlan:
    """Complete editing plan generated by LLM"""
    plan_id: str
    video_path: str
    goal: str  # User's editing goal
    decisions: List[EditDecision] = field(default_factory=list)
    estimated_duration: float = 0.0
    quality_threshold: float = 0.7
    max_iterations: int = 3
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "plan_id": self.plan_id,
            "video_path": self.video_path,
            "goal": self.goal,
            "decisions": [d.to_dict() for d in self.decisions],
            "estimated_duration": self.estimated_duration,
            "quality_threshold": self.quality_threshold,
            "max_iterations": self.max_iterations,
            "metadata": self.metadata
        }

    def save_json(self, output_path: str):
        """Save plan to JSON"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
        logger.info(f"Saved edit plan to: {output_path}")


class LLMDecisionEngine:
    """
    LLM Decision Engine for Autonomous Video Editing

    This is the CORE INNOVATION of Module 8.

    The LLM:
    1. Analyzes video content (using Module 7 analysis)
    2. Understands user's creative intent
    3. Makes editing decisions (cut, speed, effects, etc.)
    4. Evaluates results and iterates if quality is below threshold

    Key Features:
    - Zero human intervention (fully autonomous)
    - Quality-driven iteration
    - Creative decision-making based on content understanding
    - Integration with all video analysis and editing tools

    Usage:
        async with LLMDecisionEngine() as engine:
            plan = await engine.create_edit_plan(
                video_path="video.mp4",
                goal="Create a funny 30-second highlight reel",
                analysis_results=video_analysis
            )
    """

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        model: str = "qwen-14b",
        temperature: float = 0.7
    ):
        """
        Initialize LLM decision engine

        Args:
            llm_client: LLM client (None = create new)
            model: LLM model to use
            temperature: Creativity temperature
        """
        self._llm_client = llm_client
        self._own_client = llm_client is None
        self.model = model
        self.temperature = temperature

        logger.info(f"LLMDecisionEngine initialized (model={model}, temp={temperature})")

    async def __aenter__(self):
        """Async context manager entry"""
        if self._own_client:
            self._llm_client = LLMClient()
            await self._llm_client.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self._own_client and self._llm_client:
            await self._llm_client.__aexit__(exc_type, exc_val, exc_tb)

    async def create_edit_plan(
        self,
        video_path: str,
        goal: str,
        analysis_results: Optional[Dict[str, Any]] = None,
        constraints: Optional[List[str]] = None,
        target_duration: Optional[float] = None
    ) -> EditPlan:
        """
        Create comprehensive editing plan using LLM

        Args:
            video_path: Path to video file
            goal: User's editing goal (e.g., "Create a funny highlight reel")
            analysis_results: Video analysis results from Module 7
            constraints: List of constraints (e.g., "Keep all dialogue", "Max 30 seconds")
            target_duration: Target video duration in seconds

        Returns:
            EditPlan
        """
        logger.info(f"Creating edit plan for: {video_path}")
        logger.info(f"Goal: {goal}")

        # Build context for LLM
        context = self._build_analysis_context(video_path, analysis_results)

        # Build prompt
        prompt = self._build_planning_prompt(
            video_path, goal, context, constraints, target_duration
        )

        # Get LLM decision
        response = await self._llm_client.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=2000
        )

        # Parse LLM response into EditPlan
        plan = self._parse_edit_plan(
            response["content"],
            video_path,
            goal,
            analysis_results
        )

        logger.info(f"Edit plan created: {len(plan.decisions)} decisions")

        return plan

    def _build_analysis_context(
        self,
        video_path: str,
        analysis_results: Optional[Dict[str, Any]]
    ) -> str:
        """Build context string from video analysis results"""
        if not analysis_results:
            return "No video analysis available."

        context_parts = []

        # Scene information
        if "scene_detection" in analysis_results:
            scenes = analysis_results["scene_detection"]
            context_parts.append(
                f"Video has {scenes.get('total_scenes', 0)} scenes with "
                f"average duration {scenes.get('avg_scene_duration', 0):.2f}s"
            )

        # Composition quality
        if "composition" in analysis_results:
            comp = analysis_results["composition"]
            context_parts.append(
                f"Composition quality: {comp.get('avg_composition_score', 0):.3f} "
                f"(best frame: {comp.get('best_frame', 0)})"
            )

        # Camera style
        if "camera_tracking" in analysis_results:
            camera = analysis_results["camera_tracking"]
            context_parts.append(
                f"Camera style: {camera.get('camera_style', 'unknown')} "
                f"({camera.get('total_shots', 0)} shots)"
            )

        # Temporal quality
        if "temporal_coherence" in analysis_results:
            temporal = analysis_results["temporal_coherence"]
            context_parts.append(
                f"Temporal stability: {temporal.get('temporal_stability_rating', 'unknown')} "
                f"(coherence: {temporal.get('avg_coherence_score', 0):.3f})"
            )

        return "\n".join(context_parts)

    def _build_planning_prompt(
        self,
        video_path: str,
        goal: str,
        context: str,
        constraints: Optional[List[str]],
        target_duration: Optional[float]
    ) -> str:
        """Build prompt for LLM editing plan"""
        constraints_text = "\n".join(f"- {c}" for c in (constraints or []))
        target_text = f"\nTarget duration: {target_duration:.1f} seconds" if target_duration else ""

        prompt = f"""You are an expert video editor. Analyze the following video and create a detailed editing plan.

VIDEO ANALYSIS:
{context}

USER'S GOAL:
{goal}

CONSTRAINTS:
{constraints_text if constraints else "None"}
{target_text}

Create a JSON editing plan with the following structure:
{{
  "decisions": [
    {{
      "decision_id": "cut_001",
      "decision_type": "cut",
      "confidence": 0.9,
      "reasoning": "This scene has the best composition and is most relevant to the goal",
      "parameters": {{
        "start_time": 10.5,
        "end_time": 25.3
      }},
      "priority": 8
    }},
    {{
      "decision_id": "speed_001",
      "decision_type": "speed",
      "confidence": 0.8,
      "reasoning": "Speed up boring parts to maintain energy",
      "parameters": {{
        "start_time": 30.0,
        "end_time": 40.0,
        "speed_factor": 2.0
      }},
      "priority": 6
    }}
  ],
  "quality_threshold": 0.7,
  "max_iterations": 3
}}

Available decision types:
- "cut": Extract specific segment (start_time, end_time)
- "speed": Change playback speed (start_time, end_time, speed_factor)
- "composite": Overlay layers (foreground_path, background_path, position)
- "effect": Apply visual effect (effect_type, parameters)
- "transition": Add transition between clips (transition_type, duration)
- "text_overlay": Add text (text, position, duration)

IMPORTANT:
1. Prioritize decisions based on their importance to the goal
2. Be specific with timestamps and parameters
3. Provide clear reasoning for each decision
4. Consider the video's composition, camera work, and temporal quality
5. Aim for the target duration if specified

Respond ONLY with valid JSON, no additional text.
"""

        return prompt

    def _parse_edit_plan(
        self,
        llm_response: str,
        video_path: str,
        goal: str,
        analysis_results: Optional[Dict[str, Any]]
    ) -> EditPlan:
        """Parse LLM response into EditPlan"""
        try:
            # Parse JSON
            response_json = json.loads(llm_response)

            # Extract decisions
            decisions = []
            for i, dec_data in enumerate(response_json.get("decisions", [])):
                decision = EditDecision(
                    decision_id=dec_data.get("decision_id", f"decision_{i:03d}"),
                    decision_type=dec_data.get("decision_type", "unknown"),
                    confidence=float(dec_data.get("confidence", 0.5)),
                    reasoning=dec_data.get("reasoning", ""),
                    parameters=dec_data.get("parameters", {}),
                    priority=int(dec_data.get("priority", 5))
                )
                decisions.append(decision)

            # Sort by priority (highest first)
            decisions.sort(key=lambda d: d.priority, reverse=True)

            # Create plan
            plan = EditPlan(
                plan_id=f"plan_{int(time.time())}",
                video_path=video_path,
                goal=goal,
                decisions=decisions,
                quality_threshold=float(response_json.get("quality_threshold", 0.7)),
                max_iterations=int(response_json.get("max_iterations", 3)),
                metadata={
                    "analysis_results": analysis_results,
                    "llm_model": self.model,
                    "creation_time": time.time()
                }
            )

            return plan

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            logger.error(f"Response: {llm_response[:500]}")

            # Return empty plan
            return EditPlan(
                plan_id=f"plan_{int(time.time())}",
                video_path=video_path,
                goal=goal,
                decisions=[],
                metadata={"parse_error": str(e)}
            )

    async def evaluate_edit_quality(
        self,
        original_video_path: str,
        edited_video_path: str,
        edit_plan: EditPlan,
        analysis_results: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate quality of edited video using LLM

        Args:
            original_video_path: Original video path
            edited_video_path: Edited video path
            edit_plan: The edit plan that was executed
            analysis_results: Analysis of edited video

        Returns:
            Quality evaluation dict with score and feedback
        """
        logger.info(f"Evaluating edit quality: {edited_video_path}")

        # Build evaluation context
        context = self._build_analysis_context(edited_video_path, analysis_results)

        # Build evaluation prompt
        prompt = f"""You are a video quality expert. Evaluate the quality of this edited video.

ORIGINAL GOAL:
{edit_plan.goal}

EDIT OPERATIONS PERFORMED:
{len(edit_plan.decisions)} decisions were applied:
{chr(10).join(f"- {d.decision_type}: {d.reasoning}" for d in edit_plan.decisions[:5])}

EDITED VIDEO ANALYSIS:
{context}

EVALUATION CRITERIA:
1. Does the edit achieve the stated goal?
2. Is the video quality acceptable (composition, temporal coherence)?
3. Are the cuts and transitions smooth?
4. Is the pacing appropriate?
5. Are there any obvious issues or artifacts?

Respond with a JSON evaluation:
{{
  "overall_quality_score": 0.85,
  "goal_achievement": 0.9,
  "technical_quality": 0.8,
  "creative_quality": 0.85,
  "needs_improvement": false,
  "feedback": "The edit successfully creates an engaging highlight reel. Composition is good, but one transition could be smoother.",
  "suggested_improvements": [
    "Soften transition at 15s",
    "Slightly increase speed of middle section"
  ]
}}

Respond ONLY with valid JSON.
"""

        # Get LLM evaluation
        response = await self._llm_client.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,  # Lower temperature for consistent evaluation
            max_tokens=1000
        )

        try:
            evaluation = json.loads(response["content"])
            logger.info(f"Quality score: {evaluation.get('overall_quality_score', 0):.3f}")
            return evaluation

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse evaluation: {e}")
            return {
                "overall_quality_score": 0.5,
                "needs_improvement": True,
                "feedback": "Evaluation failed",
                "error": str(e)
            }

    async def suggest_improvements(
        self,
        edit_plan: EditPlan,
        quality_evaluation: Dict[str, Any]
    ) -> List[EditDecision]:
        """
        Suggest improvements based on quality evaluation

        Args:
            edit_plan: Original edit plan
            quality_evaluation: Quality evaluation results

        Returns:
            List of new/modified decisions
        """
        logger.info("Suggesting improvements based on evaluation")

        if quality_evaluation.get("overall_quality_score", 0) >= edit_plan.quality_threshold:
            logger.info("Quality threshold met, no improvements needed")
            return []

        # Build improvement prompt
        feedback = quality_evaluation.get("feedback", "")
        suggestions = quality_evaluation.get("suggested_improvements", [])

        prompt = f"""The edited video needs improvement. Suggest specific editing changes.

ORIGINAL GOAL:
{edit_plan.goal}

QUALITY EVALUATION:
Score: {quality_evaluation.get('overall_quality_score', 0):.3f}
Feedback: {feedback}

SUGGESTED IMPROVEMENTS:
{chr(10).join(f"- {s}" for s in suggestions)}

ORIGINAL DECISIONS:
{chr(10).join(f"- {d.decision_id}: {d.decision_type} ({d.reasoning})" for d in edit_plan.decisions[:5])}

Create NEW or MODIFIED editing decisions to address the issues. Respond with JSON:
{{
  "new_decisions": [
    {{
      "decision_id": "improve_001",
      "decision_type": "transition",
      "confidence": 0.85,
      "reasoning": "Add smooth crossfade to address jarring cut",
      "parameters": {{
        "clip1_end": 14.5,
        "clip2_start": 15.0,
        "transition_type": "crossfade",
        "duration": 0.5
      }},
      "priority": 7
    }}
  ]
}}

Respond ONLY with valid JSON.
"""

        response = await self._llm_client.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1500
        )

        try:
            improvements = json.loads(response["content"])
            new_decisions = []

            for dec_data in improvements.get("new_decisions", []):
                decision = EditDecision(
                    decision_id=dec_data.get("decision_id", f"improve_{len(new_decisions)}"),
                    decision_type=dec_data.get("decision_type", "unknown"),
                    confidence=float(dec_data.get("confidence", 0.5)),
                    reasoning=dec_data.get("reasoning", ""),
                    parameters=dec_data.get("parameters", {}),
                    priority=int(dec_data.get("priority", 5)),
                    metadata={"improvement": True}
                )
                new_decisions.append(decision)

            logger.info(f"Suggested {len(new_decisions)} improvements")
            return new_decisions

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse improvements: {e}")
            return []


async def main():
    """Example usage"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Example: Create edit plan
    async with LLMDecisionEngine() as engine:
        # Mock analysis results
        analysis_results = {
            "scene_detection": {
                "total_scenes": 10,
                "avg_scene_duration": 5.2
            },
            "composition": {
                "avg_composition_score": 0.75,
                "best_frame": 245
            },
            "camera_tracking": {
                "camera_style": "dynamic",
                "total_shots": 8
            },
            "temporal_coherence": {
                "temporal_stability_rating": "good",
                "avg_coherence_score": 0.82
            }
        }

        # Create edit plan
        plan = await engine.create_edit_plan(
            video_path="test_video.mp4",
            goal="Create a funny 30-second highlight reel with fast-paced action",
            analysis_results=analysis_results,
            constraints=["Keep dialogue clear", "Maximum 30 seconds"],
            target_duration=30.0
        )

        print("\n" + "=" * 60)
        print("LLM EDIT PLAN")
        print("=" * 60)
        print(f"Goal: {plan.goal}")
        print(f"Decisions: {len(plan.decisions)}")
        print(f"Quality Threshold: {plan.quality_threshold}")

        for i, decision in enumerate(plan.decisions, 1):
            print(f"\n{i}. {decision.decision_type.upper()} (Priority: {decision.priority})")
            print(f"   Confidence: {decision.confidence:.2f}")
            print(f"   Reasoning: {decision.reasoning}")
            print(f"   Parameters: {decision.parameters}")

        # Save plan
        plan.save_json("outputs/editing/edit_plan.json")


if __name__ == "__main__":
    asyncio.run(main())
