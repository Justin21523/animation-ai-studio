# Animation AI Studio

**Advanced LLM-Driven AI Platform for 3D Animation Creation**

[![Status](https://img.shields.io/badge/Status-Week%203--4%20In%20Progress-yellow)](docs/reports/project-milestones.md)
[![Phase](https://img.shields.io/badge/Phase-25%25%20Complete-blue)](docs/reports/project-milestones.md)

---

## ğŸ¯ Overview

**Animation AI Studio** is an advanced multimodal AI platform that uses **open-source LLM agents** as the core decision-making engine to create, analyze, and transform 3D animated content (Pixar/Disney-style).

### Core Architecture: LLM + RAG + Agent (ç¼ºä¸€ä¸å¯)

```
Week 7-8: AI Video Editing (å¤§å£“è»¸) - AI è‡ªä¸»å‰µä½œå½±ç‰‡
    â†“
Week 5-6: LangGraph Agent + RAG - LLM ç†è§£æ„åœ– + RAG æª¢ç´¢è³‡æ–™ + Agent æ±ºç­–
    â†“
Week 3-4: 3D Character Tools - SDXL + LoRA + ControlNet + GPT-SoVITS (IN PROGRESS)
    â†“
Week 1-2: LLM Backend - vLLM + FastAPI + Redis + Docker (COMPLETE âœ…)
```

### Key Features

- **LLM Decision Engine**: Qwen2.5-VL-7B, Qwen2.5-14B, Qwen2.5-Coder-7B (self-hosted)
- **Image Generation**: SDXL + LoRA + ControlNet (character, pose, style)
- **Voice Synthesis**: GPT-SoVITS (voice cloning, emotion control)
- **Agent Framework**: LangGraph + RAG (autonomous creative decisions)
- **Video Editing**: AI-powered parody generation and effects

---

## ğŸš€ Quick Start

### For New Claude Code Sessions

**English:** See [docs/guides/claude-code-onboarding.md](docs/guides/claude-code-onboarding.md)

**ç¹é«”ä¸­æ–‡ï¼š** è¦‹ [docs/guides/claude-code-onboarding.md](docs/guides/claude-code-onboarding.md)

### For Project Context

1. **[docs/architecture/project-architecture.md](docs/architecture/project-architecture.md)** - Overall architecture and implementation plan
2. **[CLAUDE.md](CLAUDE.md)** - Complete project instructions
3. **[docs/reports/project-milestones.md](docs/reports/project-milestones.md)** - Current progress

---

## ğŸ“Š Current Status

**Phase:** Week 3-4 - 3D Character Generation Tools (IN PROGRESS)

**Progress:** 25% Complete (Week 1-2 of 8)

| Week | Goal | Status |
|------|------|--------|
| 1-2 | LLM Backend Foundation | âœ… COMPLETE |
| 3-4 | 3D Character Tools | ğŸ”„ IN PROGRESS |
| 5-6 | Agent Framework | ğŸ“‹ PENDING |
| 7-8 | Integration (å¤§å£“è»¸) | ğŸ“‹ PENDING |

**Details:** See [docs/reports/project-milestones.md](docs/reports/project-milestones.md)

---

## ğŸ–¥ï¸ Hardware Configuration

**CRITICAL:** RTX 5080 16GB VRAM (single GPU)

```yaml
CPU: AMD Ryzen 9 9950X (16 cores)
RAM: 64GB DDR5
GPU: NVIDIA RTX 5080 16GB VRAM
PyTorch: 2.7.0 + CUDA 12.8 (IMMUTABLE)
Environment: conda ai_env
```

**Constraints:**
- Only ONE heavy model at a time (LLM OR SDXL)
- Dynamic model switching supported (20-35s)
- PyTorch SDPA only (xformers FORBIDDEN)

---

## ğŸ—‚ï¸ Project Structure

```
animation-ai-studio/
â”œâ”€â”€ docs/                       # ğŸ“š All documentation
â”‚   â”œâ”€â”€ architecture/           # Project architecture and design
â”‚   â”œâ”€â”€ guides/                 # User guides and onboarding
â”‚   â”œâ”€â”€ reports/                # Weekly completion reports
â”‚   â””â”€â”€ reference/              # Technical reference
â”œâ”€â”€ llm_backend/                # âœ… Week 1-2: LLM services
â”‚   â”œâ”€â”€ gateway/                # FastAPI Gateway
â”‚   â”œâ”€â”€ services/               # vLLM configurations
â”‚   â”œâ”€â”€ docker/                 # Docker orchestration
â”‚   â””â”€â”€ scripts/                # Management scripts
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ core/                   # Shared utilities
â”‚   â”‚   â”œâ”€â”€ llm_client/         # âœ… LLM client
â”‚   â”‚   â””â”€â”€ generation/         # ğŸ”„ Model manager (Week 3-4)
â”‚   â”œâ”€â”€ generation/             # ğŸ”„ Image generation (Week 3-4)
â”‚   â”œâ”€â”€ synthesis/              # ğŸ”„ Voice synthesis (Week 3-4)
â”‚   â”œâ”€â”€ ai_editing/             # ğŸ“‹ Agent framework (Week 5-8)
â”‚   â”œâ”€â”€ analysis/               # Video, audio, image analysis
â”‚   â””â”€â”€ applications/           # End-user applications
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ generation/             # ğŸ”„ Generation configs (Week 3-4)
â”‚   â””â”€â”€ agent/                  # ğŸ“‹ Agent configs (Week 5-6)
â”œâ”€â”€ data/films/                 # Character metadata (shared with LoRA pipeline)
â”œâ”€â”€ outputs/                    # Generated content
â”œâ”€â”€ requirements/               # Modular dependencies
â”œâ”€â”€ CLAUDE.md                   # Complete project instructions
â”œâ”€â”€ OPEN_SOURCE_MODELS.md       # Models and tools reference
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“‚ Data Sources

### Shared Film Datasets

**Location:** `/mnt/data/ai_data/datasets/3d-anime/`

- Films: luca, coco, elio, onward, orion, turning-red, up
- Content: frames, audio, metadata
- Shared with 3D Animation LoRA Pipeline

### AI Warehouse

**Location:** `/mnt/c/AI_LLM_projects/ai_warehouse/`

```
models/
â”œâ”€â”€ llm/           # LLM models (Week 1-2)
â”œâ”€â”€ diffusion/     # SDXL, ControlNet (Week 3-4)
â”œâ”€â”€ tts/           # GPT-SoVITS models (Week 3-4)
â””â”€â”€ cv/            # Computer vision models

cache/
â”œâ”€â”€ huggingface/
â”œâ”€â”€ vllm/
â””â”€â”€ diffusers/
```

---

## ğŸ¬ Usage Examples

### Week 1-2: LLM Backend (READY âœ…)

```bash
# Start LLM services (interactive model selection)
bash llm_backend/scripts/start_all.sh

# Check health
bash llm_backend/scripts/health_check.sh

# Python client usage
python -c "
from scripts.core.llm_client import LLMClient
import asyncio

async def main():
    async with LLMClient() as client:
        response = await client.chat(
            model='qwen-14b',
            messages=[{'role': 'user', 'content': 'Explain AI'}]
        )
        print(response)

asyncio.run(main())
"
```

### Week 3-4: Character Generation (IN PROGRESS ğŸ”„)

```python
# Image generation (coming soon)
from scripts.generation.image import CharacterGenerator

generator = CharacterGenerator()
result = await generator.generate_character(
    character="luca",
    scene="running on the beach, excited expression",
    quality="high"
)

# Voice synthesis (coming soon)
from scripts.synthesis.tts import GPTSoVITSWrapper

synthesizer = GPTSoVITSWrapper()
audio = await synthesizer.synthesize(
    text="Silenzio, Bruno!",
    character="luca",
    emotion="excited"
)
```

---

## ğŸ“š Documentation

### Essential Reading

1. **[CLAUDE.md](CLAUDE.md)** - Complete project instructions for Claude Code
2. **[docs/architecture/project-architecture.md](docs/architecture/project-architecture.md)** - Overall architecture
3. **[docs/guides/claude-code-onboarding.md](docs/guides/claude-code-onboarding.md)** - Quick start guide
4. **[OPEN_SOURCE_MODELS.md](OPEN_SOURCE_MODELS.md)** - Models reference

### Implementation Guides

- **[docs/reports/week-1-2-completion.md](docs/reports/week-1-2-completion.md)** - Week 1-2 completion report
- **[docs/reports/week-3-4-plan.md](docs/reports/week-3-4-plan.md)** - Week 3-4 implementation plan
- **[docs/reports/project-milestones.md](docs/reports/project-milestones.md)** - Progress tracking

### Technical Reference

- **[docs/architecture/llm-backend.md](docs/architecture/llm-backend.md)** - LLM backend architecture
- **[llm_backend/README.md](llm_backend/README.md)** - LLM backend usage guide
- **[llm_backend/HARDWARE_SPECS.md](llm_backend/HARDWARE_SPECS.md)** - Hardware specifications

---

## ğŸ”— Related Projects

### 3D Animation LoRA Pipeline

**Location:** `/mnt/c/AI_LLM_projects/3d-animation-lora-pipeline`

**Purpose:** Train LoRA adapters for character/background/pose generation

**Current Status:**
- Luca SAM2 segmentation: 14.8% (ç´„ 43h remaining)
- Next: LaMa inpainting â†’ Batch process 6 films

**Integration:**
- Trained LoRAs will be loaded via `configs/generation/lora_registry.yaml`
- Character metadata shared via `data/films/`

---

## âš ï¸ Critical Requirements

### MUST Use (Open-Source Only)

- âœ… Qwen2.5-VL, Qwen2.5-14B (LLM)
- âœ… vLLM (self-hosted backend)
- âœ… SDXL + LoRA (image generation)
- âœ… GPT-SoVITS (voice synthesis)
- âœ… LangGraph (agent framework)
- âœ… PyTorch 2.7.0 + CUDA 12.8

### MUST NOT Use

- âŒ Ollama (we use vLLM)
- âŒ GPT-4, Claude, Gemini (closed-source)
- âŒ Any paid APIs
- âŒ xformers (breaks PyTorch compatibility)
- âŒ Modify PyTorch or CUDA versions

---

## ğŸ“ Key Concepts

### LLM as Creative Brain

Not just a tool - LLM makes creative decisions:
- Understands artistic intent
- Plans execution steps
- Selects appropriate tools
- Evaluates quality
- Iterates until perfect

### RAG for Context

Retrieves relevant information:
- Character descriptions
- Style guides
- Past generations
- Film analysis

### Agent for Automation

Autonomous workflow execution:
- Tool calling
- Multi-step planning
- Quality-driven iteration
- Self-improvement

---

## ğŸ“ Getting Help

**For New Sessions:** [docs/guides/claude-code-onboarding.md](docs/guides/claude-code-onboarding.md)

**For Architecture:** [docs/architecture/project-architecture.md](docs/architecture/project-architecture.md)

**For Current Status:** [docs/reports/project-milestones.md](docs/reports/project-milestones.md)

**For Models:** [OPEN_SOURCE_MODELS.md](OPEN_SOURCE_MODELS.md)

---

## ğŸ“Š Progress

**Version:** v0.2.0
**Last Updated:** 2025-11-16
**Current Phase:** Week 3-4 (3D Character Tools)
**Completion:** 25% (Week 1-2 of 8)

**See [docs/reports/project-milestones.md](docs/reports/project-milestones.md) for detailed progress tracking.**

---

## ğŸ“„ License

Internal research project.
