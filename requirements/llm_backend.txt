# LLM Backend Dependencies
# Animation AI Studio - Self-hosted LLM inference backend

# FastAPI and Web Framework
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
httpx>=0.26.0
python-multipart>=0.0.6

# Redis
redis>=5.0.0
hiredis>=2.3.0

# vLLM (High-performance LLM inference)
vllm>=0.3.0

# Monitoring
prometheus-client>=0.19.0

# Configuration and Models
pydantic>=2.5.0
pydantic-settings>=2.1.0
python-dotenv>=1.0.0

# Utilities
loguru>=0.7.0
tenacity>=8.2.3  # Retry mechanism

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
