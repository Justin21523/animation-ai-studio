ä¸‹é¢é€™æ®µä½ å¯ä»¥ç›´æ¥ä¸Ÿçµ¦ Claude Code / DeepSeek / ä»»ä½• code agent ç•¶ system / instructionsï¼Œç”¨ä¾†è®“å®ƒå€‘**ä¸è¦äº‚å¡æª”æ¡ˆ**ï¼Œè€Œæ˜¯ä¹–ä¹–ç…§ä½ ç¾åœ¨çš„ AI_WAREHOUSE 3.0 çµæ§‹ä¾†æ“ä½œã€‚

---

## ğŸ§© Filesystem & Storage Spec for Code Agents

> **Context:** This machine is an Ubuntu-based AI workstation with three NVMe drives.
> All code, datasets and models **must** follow the layout below.
> Do **not** write large files under `$HOME` unless explicitly allowed.

### 1. Disks & Mount Points

* **System disk (1 TB)**

  * Device: `/dev/nvme0n1`
  * Mount: `/`
  * Usage: OS, small system tools only.
  * âŒ Do **not** store datasets or large models under `/` or `$HOME` by default.

* **Fast SSD for models & code (2 TB)**

  * Device: `/dev/nvme1n1p1`
  * Mount: `/mnt/c`
  * Usage: models, code projects, tools, conda/envs, caches.
  * This is the primary place for anything â€œmodel-ishâ€ or â€œcode-ishâ€.

* **Large SSD for datasets & training outputs (4 TB)**

  * Device: `/dev/nvme2n1p1`
  * Mount: `/mnt/data`
  * Usage: datasets, extracted frames, segmentation masks, training outputs, large media.

---

### 2. Directory Layout â€“ `/mnt/c` (2 TB, models & code)

All **models, tools, projects, caches** must live here:

```text
/mnt/c
 â”œâ”€â”€ ai_models/          # All model weights / LoRAs / checkpoints
 â”‚   â”œâ”€â”€ clip/
 â”‚   â”œâ”€â”€ controlnet/
 â”‚   â”œâ”€â”€ detection/
 â”‚   â”œâ”€â”€ embeddings/
 â”‚   â”œâ”€â”€ flow/
 â”‚   â”œâ”€â”€ inpainting/
 â”‚   â”œâ”€â”€ llm/
 â”‚   â”œâ”€â”€ reranker/
 â”‚   â”œâ”€â”€ safety/
 â”‚   â”œâ”€â”€ segmentation/
 â”‚   â”œâ”€â”€ stable-diffusion/
 â”‚   â”œâ”€â”€ video/
 â”‚   â”œâ”€â”€ lora/
 â”‚   â””â”€â”€ lora_sdxl/
 â”‚
 â”œâ”€â”€ ai_projects/        # Git / coding projects (repos, apps, scripts)
 â”‚   â””â”€â”€ <project_name>/
 â”‚
 â”œâ”€â”€ ai_tools/           # Standalone tools
 â”‚   â”œâ”€â”€ kohya_ss/
 â”‚   â”œâ”€â”€ comfyui/
 â”‚   â”œâ”€â”€ sd_scripts/
 â”‚   â””â”€â”€ rvc/
 â”‚
 â”œâ”€â”€ ai_envs/            # (optional) conda / venv dirs if we decide to move them here
 â”‚
 â”œâ”€â”€ ai_cache/           # All AI-related caches MUST go here (not under $HOME)
 â”‚   â”œâ”€â”€ huggingface/
 â”‚   â”œâ”€â”€ pip/
 â”‚   â””â”€â”€ torch/
 â”‚
 â””â”€â”€ tmp/                # scratch space for temporary downloads / unpacking
```

#### Environment variables (for any code you write):

When configuring Python, HF, transformers, etc, always assume:

```bash
HF_HOME=/mnt/c/ai_cache/huggingface
TRANSFORMERS_CACHE=/mnt/c/ai_cache/huggingface
TORCH_HOME=/mnt/c/ai_cache/torch
XDG_CACHE_HOME=/mnt/c/ai_cache
```

If you generate scripts / notebooks, please **set these** so no large cache goes to `$HOME/.cache` or `/tmp`.

---

### 3. Directory Layout â€“ `/mnt/data` (4 TB, datasets & training)

All **datasets, training runs, extracted media** go here:

```text
/mnt/data
 â”œâ”€â”€ datasets/
 â”‚   â”œâ”€â”€ pixar/
 â”‚   â”œâ”€â”€ elio/
 â”‚   â”œâ”€â”€ luca/
 â”‚   â”œâ”€â”€ audio/
 â”‚   â”œâ”€â”€ video/
 â”‚   â”œâ”€â”€ web/
 â”‚   â”œâ”€â”€ general/              # misc datasets (3d-anime, hunter, inazuma-eleven, yokai-watch, etc.)
 â”‚   â””â”€â”€ medical/
 â”‚        â”œâ”€â”€ aicup_2025_heart_seg/
 â”‚        â””â”€â”€ nnUNet_raw/
 â”‚             â””â”€â”€ Dataset001_HeartSeg/
 â”‚
 â”œâ”€â”€ training/
 â”‚   â”œâ”€â”€ lora/
 â”‚   â”‚   â”œâ”€â”€ expression_lora/
 â”‚   â”‚   â””â”€â”€ evaluation/       # migrated from ai_data/lora_evaluation/*
 â”‚   â”œâ”€â”€ sd_finetune/
 â”‚   â”œâ”€â”€ controlnet/
 â”‚   â”œâ”€â”€ runs/
 â”‚   â”œâ”€â”€ logs/                 # migrated from ai_data/logs/*
 â”‚   â””â”€â”€ nnunet/
 â”‚        â””â”€â”€ aicup_2025_heart_seg/
 â”‚
 â”œâ”€â”€ extracted/
 â”‚   â”œâ”€â”€ frames/
 â”‚   â”œâ”€â”€ captions/
 â”‚   â””â”€â”€ sam_masks/
 â”‚
 â”œâ”€â”€ videos/
 â”‚   â”œâ”€â”€ raw/
 â”‚   â”œâ”€â”€ processed/
 â”‚   â””â”€â”€ ytp/
 â”‚
 â”œâ”€â”€ audio/
 â”‚   â”œâ”€â”€ rvc_input/
 â”‚   â””â”€â”€ rvc_output/
 â”‚
 â”œâ”€â”€ backups/
 â””â”€â”€ tmp/
```

> **Important:**
> Legacy path `/mnt/data/ai_data/...` exists only as historical data;
> **new code must NOT write there**. Always write into the new structure above.

---

### 4. Rules for Code / Scripts / Agents

1. **Never default to `$HOME` for big stuff.**

   * No large datasets, models or checkpoints under `~` or `/`.
   * Use `/mnt/c` for models/tools, `/mnt/data` for datasets/outputs.

2. **Models go to `/mnt/c/ai_models` only.**

   * If you download HF models, LoRAs, checkpoints, etc, place them in the appropriate subfolder.
   * If you generate new LoRAs, save them under:

     * `/mnt/c/ai_models/lora/â€¦` (SD1.5)
     * `/mnt/c/ai_models/lora_sdxl/â€¦` (SDXL)
     * or other subfolder under `ai_models` as appropriate.

3. **Datasets and training outputs go to `/mnt/data`.**

   * New datasets â†’ `/mnt/data/datasets/...`
   * New training runs â†’ `/mnt/data/training/runs/...`
   * Logs / metrics â†’ `/mnt/data/training/logs/...`
   * Frame extraction / SAM â†’ `/mnt/data/extracted/...`

4. **Cache / temp writes should respect env vars**

   * Use `HF_HOME`, `TRANSFORMERS_CACHE`, `TORCH_HOME`, `XDG_CACHE_HOME` as above.
   * If you generate a script / Dockerfile, **inject those env vars**.

5. **Conda / venvs**

   * Default is standard `~/miniconda3/envs/...`, **but** if you deliberately create a long-lived env for a big project (e.g. `kohya_ss`), prefer placing it under `/mnt/c/ai_envs/<env_name>` and referencing it explicitly.

6. **Never hardcode absolute home-relative paths for models or datasets.**

   * Use `/mnt/c/...` and `/mnt/data/...` explicitly, or environment variables that point there.

---

### 5. Google Drive / rclone rules

Remote name: **`gdrive`**

Structure in Drive:

```text
gdrive:ai_warehouse/
 â”œâ”€â”€ logs/
 â”œâ”€â”€ train/
 â”œâ”€â”€ cache/
 â”œâ”€â”€ rag/
 â”œâ”€â”€ tool-caches/
 â”œâ”€â”€ checkpoints/
 â”œâ”€â”€ hf/
 â”œâ”€â”€ models/
 â””â”€â”€ outputs/
```

**When syncing models from Google Drive:**

* Source: `gdrive:ai_warehouse/models`
* Target: `/mnt/c/ai_models`

Example command (for reference):

```bash
rclone sync gdrive:ai_warehouse/models /mnt/c/ai_models \
  --progress --transfers=8 --checkers=16 --drive-chunk-size=256M
```

> Code agents should **not** change the rclone config or remote name.
> If needed, they may assume `gdrive` exists and follows this structure.

---

### 6. Migration note (for agents reading old code)

* Old scripts may reference paths like `/mnt/data/ai_data/datasets/...` or `/mnt/data/ai_data/models/...`.
* New code should translate those into the new layout as described above.
* Do **not** resurrect deprecated directories like `/mnt/data/ai_data` for new work.

---
