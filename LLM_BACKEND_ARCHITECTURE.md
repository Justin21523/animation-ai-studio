# LLM Backend Architecture - è‡ªå»ºæ¨ç†æœå‹™

**æ ¸å¿ƒæ±ºç­–ï¼šä¸ä½¿ç”¨ Ollamaï¼Œè‡ªå»ºå®Œæ•´çš„ LLM æœå‹™å¾Œç«¯**

---

## ğŸ—ï¸ æ¶æ§‹æ¦‚è¦½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Animation AI Studio                       â”‚
â”‚                    (Application Layer)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/WebSocket
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM Service Gateway                         â”‚
â”‚              (Load Balancing & Routing)                      â”‚
â”‚                    FastAPI + Redis                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚          â”‚          â”‚
            â–¼          â–¼          â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ vLLM    â”‚ â”‚ vLLM    â”‚ â”‚ vLLM    â”‚
      â”‚ Service â”‚ â”‚ Service â”‚ â”‚ Service â”‚
      â”‚ (Qwen)  â”‚ â”‚(DeepSeekâ”‚ â”‚ (Coder) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚          â”‚           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Model Storage      â”‚
         â”‚ (Shared Volume/NFS)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶

### 1. vLLM æ¨ç†å¼•æ“ (æ¨è–¦)

**ç‚ºä»€éº¼é¸ vLLM:**
- âš¡ PagedAttention - è¨˜æ†¶é«”æ•ˆç‡é«˜ 2-4x
- ğŸš€ Continuous batching - ååé‡é«˜ 24x
- ğŸ”Œ OpenAI API ç›¸å®¹ - æ˜“æ–¼æ•´åˆ
- ğŸ›ï¸ å‹•æ…‹æ‰¹æ¬¡å¤§å°
- ğŸ“Š å¤šGPUæ”¯æŒ
- ğŸ”§ é‡åŒ–æ”¯æŒ (FP8, INT8, INT4)

```python
# vLLM æœå‹™å•Ÿå‹•ç¯„ä¾‹
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-VL-72B-Instruct \
  --served-model-name qwen-vl-72b \
  --port 8000 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 32768 \
  --tensor-parallel-size 2 \
  --dtype auto \
  --quantization fp8
```

**æ›¿ä»£æ–¹æ¡ˆ:**
- **TGI (Text Generation Inference)** - HuggingFace å®˜æ–¹
- **TensorRT-LLM** - NVIDIA å„ªåŒ–
- **llama.cpp** - CPU æ¨ç†(å‚™ç”¨)

---

## ğŸ“ ç›®éŒ„çµæ§‹

```
animation-ai-studio/
â”œâ”€â”€ llm_backend/
â”‚   â”œâ”€â”€ gateway/                # API Gateway
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI ä¸»ç¨‹å¼
â”‚   â”‚   â”œâ”€â”€ router.py          # è·¯ç”±ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ load_balancer.py   # è² è¼‰å‡è¡¡
â”‚   â”‚   â””â”€â”€ cache.py           # Redis å¿«å–
â”‚   â”œâ”€â”€ services/              # LLM æœå‹™
â”‚   â”‚   â”œâ”€â”€ qwen_vl/           # Qwen2.5-VL æœå‹™
â”‚   â”‚   â”‚   â”œâ”€â”€ start.sh
â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ deepseek/          # DeepSeek-V3 æœå‹™
â”‚   â”‚   â”‚   â”œâ”€â”€ start.sh
â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â””â”€â”€ qwen_coder/        # Qwen2.5-Coder æœå‹™
â”‚   â”‚       â”œâ”€â”€ start.sh
â”‚   â”‚       â””â”€â”€ config.yaml
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹å­˜å„²
â”‚   â”‚   â””â”€â”€ download.sh        # ä¸‹è¼‰è…³æœ¬
â”‚   â”œâ”€â”€ monitoring/            # ç›£æ§
â”‚   â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”‚   â””â”€â”€ grafana/
â”‚   â”œâ”€â”€ docker/                # Docker é…ç½®
â”‚   â”‚   â”œâ”€â”€ vllm.Dockerfile
â”‚   â”‚   â”œâ”€â”€ gateway.Dockerfile
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â””â”€â”€ scripts/               # ç®¡ç†è…³æœ¬
â”‚       â”œâ”€â”€ start_all.sh
â”‚       â”œâ”€â”€ stop_all.sh
â”‚       â””â”€â”€ health_check.sh
â””â”€â”€ scripts/core/llm_client/   # æ‡‰ç”¨å±¤å®¢æˆ¶ç«¯
    â”œâ”€â”€ llm_client.py          # çµ±ä¸€å®¢æˆ¶ç«¯ä»‹é¢
    â””â”€â”€ models.py              # è«‹æ±‚/éŸ¿æ‡‰æ¨¡å‹
```

---

## ğŸ”§ å¯¦ç¾ç´°ç¯€

### 1. API Gateway (FastAPI)

```python
# llm_backend/gateway/main.py

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import httpx
import redis
import json
from typing import List, Dict, Optional
from pydantic import BaseModel

app = FastAPI(title="LLM Service Gateway")

# Redis å¿«å–
cache = redis.Redis(host='localhost', port=6379, db=0)

# æœå‹™è¨»å†Šè¡¨
SERVICES = {
    "qwen-vl-72b": {
        "url": "http://localhost:8000/v1",
        "type": "multimodal",
        "priority": 1
    },
    "deepseek-v3-671b": {
        "url": "http://localhost:8001/v1",
        "type": "reasoning",
        "priority": 1
    },
    "qwen-coder-32b": {
        "url": "http://localhost:8002/v1",
        "type": "code",
        "priority": 2
    }
}

class ChatRequest(BaseModel):
    model: str
    messages: List[Dict[str, str]]
    temperature: float = 0.7
    max_tokens: int = 2048
    stream: bool = False

class ChatResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[Dict]

@app.post("/v1/chat/completions")
async def chat_completion(request: ChatRequest):
    """
    çµ±ä¸€çš„èŠå¤©å®ŒæˆAPI
    ç›¸å®¹ OpenAI API æ ¼å¼
    """
    # æª¢æŸ¥å¿«å–
    cache_key = f"chat:{request.model}:{hash(str(request.messages))}"
    cached = cache.get(cache_key)
    if cached and not request.stream:
        return json.loads(cached)

    # è·¯ç”±åˆ°å°æ‡‰æœå‹™
    service = SERVICES.get(request.model)
    if not service:
        raise HTTPException(status_code=404, detail=f"Model {request.model} not found")

    # è½‰ç™¼è«‹æ±‚
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{service['url']}/chat/completions",
            json=request.dict(),
            timeout=300.0
        )

        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=response.text)

        result = response.json()

        # å¿«å–çµæœ (1å°æ™‚)
        if not request.stream:
            cache.setex(cache_key, 3600, json.dumps(result))

        return result

@app.post("/v1/embeddings")
async def create_embeddings(model: str, input: List[str]):
    """å‘é‡åµŒå…¥API"""
    # è·¯ç”±åˆ°å¤šæ¨¡æ…‹æ¨¡å‹
    service = SERVICES.get("qwen-vl-72b")
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{service['url']}/embeddings",
            json={"model": model, "input": input}
        )
        return response.json()

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    health = {}
    async with httpx.AsyncClient() as client:
        for name, service in SERVICES.items():
            try:
                response = await client.get(f"{service['url']}/health", timeout=5.0)
                health[name] = "healthy" if response.status_code == 200 else "unhealthy"
            except:
                health[name] = "down"
    return health

@app.get("/models")
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            {
                "id": name,
                "object": "model",
                "type": info["type"]
            }
            for name, info in SERVICES.items()
        ]
    }
```

### 2. vLLM æœå‹™é…ç½®

```yaml
# llm_backend/services/qwen_vl/config.yaml

model: Qwen/Qwen2.5-VL-72B-Instruct
served_model_name: qwen-vl-72b
port: 8000

# GPU é…ç½®
gpu_memory_utilization: 0.9
tensor_parallel_size: 2  # ä½¿ç”¨2å¼µGPU
dtype: auto
quantization: fp8  # FP8 é‡åŒ–

# æ€§èƒ½é…ç½®
max_model_len: 32768
max_num_batched_tokens: 8192
max_num_seqs: 256

# å¿«å–é…ç½®
enable_prefix_caching: true
disable_log_stats: false
```

```bash
# llm_backend/services/qwen_vl/start.sh

#!/bin/bash
set -e

MODEL="Qwen/Qwen2.5-VL-72B-Instruct"
PORT=8000

echo "Starting Qwen2.5-VL-72B service on port $PORT..."

python -m vllm.entrypoints.openai.api_server \
  --model $MODEL \
  --served-model-name qwen-vl-72b \
  --port $PORT \
  --gpu-memory-utilization 0.9 \
  --max-model-len 32768 \
  --tensor-parallel-size 2 \
  --dtype auto \
  --quantization fp8 \
  --enable-prefix-caching \
  --trust-remote-code

echo "âœ… Qwen2.5-VL-72B service started"
```

### 3. DeepSeek-V3 æœå‹™é…ç½®

```bash
# llm_backend/services/deepseek/start.sh

#!/bin/bash
set -e

MODEL="deepseek-ai/DeepSeek-V3"
PORT=8001

echo "Starting DeepSeek-V3 service on port $PORT..."

# DeepSeek-V3 ä½¿ç”¨ FP8 é‡åŒ–åœ¨å–®å¡ A100 80GB é‹è¡Œ
python -m vllm.entrypoints.openai.api_server \
  --model $MODEL \
  --served-model-name deepseek-v3-671b \
  --port $PORT \
  --gpu-memory-utilization 0.95 \
  --max-model-len 65536 \
  --tensor-parallel-size 1 \
  --dtype float16 \
  --quantization fp8 \
  --enable-prefix-caching \
  --trust-remote-code \
  --max-num-seqs 128

echo "âœ… DeepSeek-V3 service started"
```

### 4. Docker Compose ç·¨æ’

```yaml
# llm_backend/docker/docker-compose.yml

version: '3.8'

services:
  # Redis å¿«å–
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # API Gateway
  gateway:
    build:
      context: ..
      dockerfile: docker/gateway.Dockerfile
    ports:
      - "7000:7000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - redis
    restart: unless-stopped

  # Qwen2.5-VL æœå‹™
  qwen-vl:
    build:
      context: ..
      dockerfile: docker/vllm.Dockerfile
    ports:
      - "8000:8000"
    environment:
      - MODEL=Qwen/Qwen2.5-VL-72B-Instruct
      - PORT=8000
      - TENSOR_PARALLEL_SIZE=2
    volumes:
      - ../models:/models
      - /dev/shm:/dev/shm  # å…±äº«è¨˜æ†¶é«”
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']  # GPU 0, 1
              capabilities: [gpu]
    restart: unless-stopped

  # DeepSeek-V3 æœå‹™
  deepseek:
    build:
      context: ..
      dockerfile: docker/vllm.Dockerfile
    ports:
      - "8001:8001"
    environment:
      - MODEL=deepseek-ai/DeepSeek-V3
      - PORT=8001
      - TENSOR_PARALLEL_SIZE=1
      - QUANTIZATION=fp8
    volumes:
      - ../models:/models
      - /dev/shm:/dev/shm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']  # GPU 2 (A100 80GB)
              capabilities: [gpu]
    restart: unless-stopped

  # Qwen2.5-Coder æœå‹™
  qwen-coder:
    build:
      context: ..
      dockerfile: docker/vllm.Dockerfile
    ports:
      - "8002:8002"
    environment:
      - MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
      - PORT=8002
      - TENSOR_PARALLEL_SIZE=1
    volumes:
      - ../models:/models
      - /dev/shm:/dev/shm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']  # GPU 3
              capabilities: [gpu]
    restart: unless-stopped

  # Prometheus ç›£æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped

  # Grafana å¯è¦–åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ../monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
```

---

## ğŸ–¥ï¸ æ‡‰ç”¨å±¤å®¢æˆ¶ç«¯

```python
# scripts/core/llm_client/llm_client.py

import httpx
import json
from typing import List, Dict, Optional, AsyncIterator
from pydantic import BaseModel

class LLMClient:
    """
    çµ±ä¸€çš„ LLM å®¢æˆ¶ç«¯
    é€£æ¥åˆ°è‡ªå»º LLM æœå‹™å¾Œç«¯
    """

    def __init__(self, base_url: str = "http://localhost:7000/v1"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=300.0)

    async def chat(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        stream: bool = False
    ) -> Dict:
        """
        ç™¼é€èŠå¤©è«‹æ±‚

        Args:
            model: æ¨¡å‹åç¨± (qwen-vl-72b, deepseek-v3-671b, qwen-coder-32b)
            messages: å°è©±æ­·å²
            temperature: æº«åº¦åƒæ•¸
            max_tokens: æœ€å¤§ç”Ÿæˆé•·åº¦
            stream: æ˜¯å¦ä¸²æµè¼¸å‡º
        """
        response = await self.client.post(
            f"{self.base_url}/chat/completions",
            json={
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": stream
            }
        )
        return response.json()

    async def understand_creative_intent(self, user_request: str) -> Dict:
        """
        ä½¿ç”¨ DeepSeek-V3 ç†è§£å‰µæ„æ„åœ–
        """
        messages = [{
            "role": "system",
            "content": "You are a creative director AI specializing in animation content creation."
        }, {
            "role": "user",
            "content": f"""Analyze this creative request in detail:

{user_request}

Provide:
1. Core creative goal
2. Desired style and mood
3. Target audience
4. Success criteria
5. Technical challenges

Return as JSON."""
        }]

        response = await self.chat(
            model="deepseek-v3-671b",
            messages=messages,
            temperature=0.3
        )

        # è§£æ JSON å›æ‡‰
        content = response['choices'][0]['message']['content']
        return json.loads(content)

    async def analyze_video_content(
        self,
        video_frames: List[str],  # base64 encoded
        analysis_focus: str
    ) -> Dict:
        """
        ä½¿ç”¨ Qwen2.5-VL åˆ†æå½±ç‰‡å…§å®¹
        """
        messages = [{
            "role": "user",
            "content": [
                {"type": "text", "text": f"Analyze this video focusing on: {analysis_focus}"},
                *[{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}}
                  for frame in video_frames[:10]]  # æœ€å¤š10å¹€
            ]
        }]

        response = await self.chat(
            model="qwen-vl-72b",
            messages=messages,
            temperature=0.2
        )

        return response

    async def generate_code(self, task_description: str) -> str:
        """
        ä½¿ç”¨ Qwen2.5-Coder ç”Ÿæˆä»£ç¢¼
        """
        messages = [{
            "role": "system",
            "content": "You are an expert Python programmer specializing in AI tools and automation."
        }, {
            "role": "user",
            "content": task_description
        }]

        response = await self.chat(
            model="qwen-coder-32b",
            messages=messages,
            temperature=0.1
        )

        return response['choices'][0]['message']['content']

    async def health_check(self) -> Dict:
        """æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹"""
        response = await self.client.get(f"{self.base_url.replace('/v1', '')}/health")
        return response.json()

# ä½¿ç”¨ç¯„ä¾‹
async def example_usage():
    client = LLMClient()

    # ç†è§£å‰µæ„æ„åœ–
    intent = await client.understand_creative_intent(
        "Create a funny parody of Luca's ocean scene"
    )
    print("Creative Intent:", intent)

    # åˆ†æå½±ç‰‡
    analysis = await client.analyze_video_content(
        video_frames=[...],  # base64 frames
        analysis_focus="comedic moments and character expressions"
    )
    print("Video Analysis:", analysis)

    # ç”Ÿæˆä»£ç¢¼
    code = await client.generate_code(
        "Write a function to apply slow-motion effect to video using MoviePy"
    )
    print("Generated Code:", code)
```

---

## ğŸš€ éƒ¨ç½²æŒ‡å—

### æœ¬åœ°éƒ¨ç½² (é–‹ç™¼ç’°å¢ƒ)

```bash
# 1. ä¸‹è¼‰æ¨¡å‹
cd llm_backend/models
bash download.sh

# 2. å•Ÿå‹•æœå‹™
cd ../
./scripts/start_all.sh

# 3. æª¢æŸ¥å¥åº·ç‹€æ…‹
./scripts/health_check.sh
```

### Docker éƒ¨ç½² (ç”Ÿç”¢ç’°å¢ƒ)

```bash
# 1. æ§‹å»ºé¡åƒ
cd llm_backend/docker
docker-compose build

# 2. å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose up -d

# 3. æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f gateway

# 4. æ“´å±•æœå‹™
docker-compose up -d --scale qwen-vl=2
```

### Kubernetes éƒ¨ç½² (å¤§è¦æ¨¡)

```yaml
# å¾…å¯¦ç¾ - K8s manifests
```

---

## ğŸ“Š ç›£æ§å’Œæ—¥èªŒ

### Prometheus æŒ‡æ¨™

```yaml
# llm_backend/monitoring/prometheus.yml

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets:
        - 'qwen-vl:8000'
        - 'deepseek:8001'
        - 'qwen-coder:8002'

  - job_name: 'gateway'
    static_configs:
      - targets: ['gateway:7000']
```

### Grafana Dashboard

- è«‹æ±‚ååé‡
- å»¶é²åˆ†ä½ˆ (P50, P95, P99)
- GPU ä½¿ç”¨ç‡
- è¨˜æ†¶é«”ä½¿ç”¨
- éŒ¯èª¤ç‡

---

## ğŸ” å®‰å…¨è€ƒæ…®

### 1. API èªè­‰

```python
# æ·»åŠ  JWT èªè­‰
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

@app.post("/v1/chat/completions")
async def chat_completion(
    request: ChatRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    # é©—è­‰ token
    verify_token(credentials.credentials)
    # ...
```

### 2. é€Ÿç‡é™åˆ¶

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/v1/chat/completions")
@limiter.limit("100/minute")
async def chat_completion(...):
    ...
```

---

## ğŸ’° æˆæœ¬ä¼°ç®—

### GPU éœ€æ±‚

```yaml
Qwen2.5-VL 72B (FP8):
  - 2x RTX 4090 (24GB each) = ~$3,200
  - æˆ– 1x A6000 (48GB) = ~$4,500
  - æˆ– é›²ç«¯ A100 40GB x2 = ~$2-3/hour

DeepSeek-V3 671B (FP8):
  - 1x A100 80GB = ~$6,000
  - æˆ– é›²ç«¯ A100 80GB = ~$3-4/hour

Qwen2.5-Coder 32B:
  - 1x RTX 4090 (24GB) = ~$1,600
  - æˆ– é›²ç«¯ A10G = ~$1/hour

Total (æœ¬åœ°):
  - ç´„ $10,000-15,000 (ä¸€æ¬¡æ€§)

Total (é›²ç«¯):
  - ç´„ $5-8/hour (æŒ‰éœ€)
```

---

## âœ… ç¸½çµ

é€™å€‹è‡ªå»º LLM å¾Œç«¯æ¶æ§‹æä¾›ï¼š

1. âœ… **å®Œå…¨è‡ªä¸»æ§åˆ¶** - ä¸ä¾è³´ Ollama
2. âœ… **é«˜æ€§èƒ½æ¨ç†** - vLLM å„ªåŒ–
3. âœ… **å½ˆæ€§æ“´å±•** - Docker/K8s æ”¯æŒ
4. âœ… **OpenAI ç›¸å®¹** - æ˜“æ–¼é·ç§»
5. âœ… **å®Œæ•´ç›£æ§** - Prometheus + Grafana
6. âœ… **è² è¼‰å‡è¡¡** - å¤šæœå‹™å”èª¿
7. âœ… **å¿«å–å„ªåŒ–** - Redis åŠ é€Ÿ

**ä¸‹ä¸€æ­¥:** é–‹å§‹å¯¦ç¾ Gateway å’Œç¬¬ä¸€å€‹ vLLM æœå‹™ï¼
