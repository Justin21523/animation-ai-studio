# Qwen2.5-Coder-7B vLLM Service Configuration
# Optimized for code generation on RTX 5080 16GB VRAM

# Model Configuration
model:
  # Model path
  path: "Qwen/Qwen2.5-Coder-7B-Instruct"
  # Alternative: Use AI warehouse
  # path: "/mnt/c/AI_LLM_projects/ai_warehouse/models/llm/qwen/Qwen2.5-Coder-7B-Instruct"

  # Served model name
  served_name: "qwen-coder-7b"

  # Trust remote code
  trust_remote_code: true

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8002
  api_key: null
  served_model_name: "qwen-coder-7b"

# GPU Configuration
# RTX 5080 16GB
gpu:
  # Conservative memory usage (~14GB)
  memory_utilization: 0.85

  # Single GPU
  tensor_parallel_size: 1

  # Pipeline parallelism
  pipeline_parallel_size: 1

  # Data type
  dtype: "auto"

  # No quantization needed for 7B on 16GB
  quantization: null

# Performance Configuration
performance:
  # Standard context length for code
  max_model_len: 32768

  # Batch size for code generation
  max_num_batched_tokens: 8192

  # Concurrent sequences
  max_num_seqs: 256

  # Block size
  block_size: 16

  # Swap space
  swap_space: 4

# Caching Configuration
cache:
  # Prefix caching useful for repeated code patterns
  enable_prefix_caching: true

  # Cache dtype
  cache_dtype: "auto"

# Logging
logging:
  disable_log_stats: false
  disable_log_requests: false
  log_level: "info"

# Advanced Options
advanced:
  # No chunked prefill needed for typical code lengths
  enable_chunked_prefill: false

  # Enforce eager mode
  enforce_eager: false

  # Max context for CUDA graph
  max_context_len_to_capture: 8192

# Environment Variables Override:
# - MODEL_PATH
# - TENSOR_PARALLEL_SIZE
# - GPU_MEMORY_UTILIZATION
# - MAX_MODEL_LEN
# - PORT
