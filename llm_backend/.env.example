# Environment Variables for LLM Backend
# Copy to .env and adjust as needed

# =============================================================================
# CRITICAL: PyTorch Configuration (DO NOT MODIFY)
# =============================================================================
# PyTorch version: 2.7.0
# CUDA version: 12.8
# Conda env: ai_env

# ABSOLUTELY FORBIDDEN: xformers
# MUST USE: PyTorch native SDPA (Scaled Dot Product Attention)
VLLM_ATTENTION_BACKEND=TORCH_SDPA
XFORMERS_DISABLED=1
VLLM_USE_TRITON_FLASH_ATTN=0

# Force PyTorch SDPA
PYTORCH_ENABLE_MPS_FALLBACK=0
TORCH_CUDNN_V8_API_ENABLED=1

# =============================================================================
# GPU Configuration (RTX 5080 16GB)
# =============================================================================
CUDA_VISIBLE_DEVICES=0
CUDA_DEVICE_ORDER=PCI_BUS_ID

# Memory optimization
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True

# VRAM utilization (90% of 16GB)
GPU_MEMORY_UTILIZATION=0.90

# =============================================================================
# Cache Paths (Shared AI Warehouse)
# =============================================================================
HF_HOME=/mnt/c/AI_LLM_projects/ai_warehouse/cache/huggingface
TRANSFORMERS_CACHE=/mnt/c/AI_LLM_projects/ai_warehouse/cache/huggingface
TORCH_HOME=/mnt/c/AI_LLM_projects/ai_warehouse/cache/torch
VLLM_CACHE_ROOT=/mnt/c/AI_LLM_projects/ai_warehouse/cache/vllm

# =============================================================================
# Model Paths
# =============================================================================
MODEL_BASE_PATH=/mnt/c/AI_LLM_projects/ai_warehouse/models/llm

# =============================================================================
# vLLM Service Configuration
# =============================================================================
# Gateway
GATEWAY_HOST=0.0.0.0
GATEWAY_PORT=7000

# Model services
QWEN_VL_PORT=8000
QWEN_14B_PORT=8001
QWEN_CODER_PORT=8002

# Tensor parallelism (RTX 5080 single GPU)
TENSOR_PARALLEL_SIZE=1

# Data type
DTYPE=auto

# Quantization (leave empty for no quantization on 7B/14B)
QUANTIZATION=

# Performance settings
MAX_MODEL_LEN=32768
MAX_NUM_BATCHED_TOKENS=8192
MAX_NUM_SEQS=256

# Caching
ENABLE_PREFIX_CACHING=true

# =============================================================================
# Redis Configuration
# =============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=INFO
LOG_DIR=/mnt/c/AI_LLM_projects/animation-ai-studio/logs

# =============================================================================
# Safety Settings
# =============================================================================
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=300
OOM_THRESHOLD=0.95
